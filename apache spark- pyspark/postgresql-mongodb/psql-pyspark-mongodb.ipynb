{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fbe0a59-2514-441a-81a3-8dd4ef64affd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a212ae67-51c3-483c-9bd8-df1c12a4918d",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "PSQL_USER=os.environ[\"PSQL_USER\"]\n",
    "PSQL_PASSWORD=os.environ[\"PSQL_PASSWORD\"]\n",
    "MONGODB_URI=os.environ[\"MONGODB_URI\"]\n",
    "MONGODB_DATABASE=os.environ['MONGODB_DATABASE']\n",
    "MONGODB_COLLECTION=os.environ['MONGODB_COLLECTION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e6b3984-5898-4f62-9201-b07e0e3675ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, IntegerType , StringType\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d75d5fb-7f01-4f0f-987f-f92ce30a9d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Picked up _JAVA_OPTIONS: -Dawt.useSystemAAFontSettings=on -Dswing.aatext=true\n",
      "Picked up _JAVA_OPTIONS: -Dawt.useSystemAAFontSettings=on -Dswing.aatext=true\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/kali/.local/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/kali/.ivy2/cache\n",
      "The jars for the packages stored in: /home/kali/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector added as a dependency\n",
      "org.postgresql#postgresql added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-5385167a-c554-49b5-89fc-db9136532541;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector;10.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.5.1 in central\n",
      "\t[4.5.1] org.mongodb#mongodb-driver-sync;[4.5.0,4.5.99)\n",
      "\tfound org.mongodb#bson;4.5.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.5.1 in central\n",
      "\tfound org.postgresql#postgresql;42.6.0 in central\n",
      "\tfound org.checkerframework#checker-qual;3.31.0 in central\n",
      ":: resolution report :: resolve 6734ms :: artifacts dl 15ms\n",
      "\t:: modules in use:\n",
      "\torg.checkerframework#checker-qual;3.31.0 from central in [default]\n",
      "\torg.mongodb#bson;4.5.1 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.5.1 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.5.1 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector;10.0.5 from central in [default]\n",
      "\torg.postgresql#postgresql;42.6.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   6   |   1   |   0   |   0   ||   6   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-5385167a-c554-49b5-89fc-db9136532541\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 6 already retrieved (0kB/44ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/11 05:25:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# create a local SparkSession\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"PYSPARK - PostgreSQL to MongoDB\")\\\n",
    "        .config(\"spark.jars.packages\",\"org.mongodb.spark:mongo-spark-connector:10.0.5,org.postgresql:postgresql:42.6.0\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e5df0a8-49df-4c63-b854-8103d8a14721",
   "metadata": {},
   "outputs": [],
   "source": [
    "listing = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql:database1\") \\\n",
    "    .option(\"dbtable\", \"listing\") \\\n",
    "    .option(\"user\", PSQL_USER) \\\n",
    "    .option(\"password\", PSQL_PASSWORD)\\\n",
    "    .option(\"driver\",\"org.postgresql.Driver\")\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dea3dd9e-e968-4da5-9b19-fc843e9dd698",
   "metadata": {},
   "outputs": [],
   "source": [
    "host=spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql:database1\") \\\n",
    "    .option(\"dbtable\", \"host\") \\\n",
    "    .option(\"user\", PSQL_USER) \\\n",
    "    .option(\"password\", PSQL_PASSWORD) \\\n",
    "    .option(\"driver\",\"org.postgresql.Driver\")\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a177d7b7-aef0-4d80-a86d-6c7426b8d4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=listing.join(host, listing.host_id==host.host_id).drop(host.host_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ef0c5e4-442a-437b-896a-8b25714c2a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/11 05:25:55 WARN CaseInsensitiveStringMap: Converting duplicated key checkpointLocation into CaseInsensitiveStringMap.\n",
      "23/04/11 05:25:55 WARN CaseInsensitiveStringMap: Converting duplicated key forcedeletetempcheckpointlocation into CaseInsensitiveStringMap.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "load=data.write\\\n",
    "        .format(\"mongodb\")\\\n",
    "        .option(\"checkpointLocation\", \"/tmp/pyspark/\")\\\n",
    "        .option(\"forceDeleteTempCheckpointLocation\", \"true\")\\\n",
    "        .option(\"connection.uri\", MONGODB_URI )\\\n",
    "        .option(\"database\", MONGODB_DATABASE )\\\n",
    "        .option(\"collection\", MONGODB_COLLECTION)\\\n",
    "        .mode(\"append\")\\\n",
    "        .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bbacf2c-79ff-4287-a7d3-8316b641d985",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
